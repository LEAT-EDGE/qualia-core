[bench]
name        = 'cifar10_VGG_fused_bias_wise_near_ICONIP_QAT8_train'
seed = 1
first_run = 1
last_run = 1

[learningframework]
kind = 'PyTorch'
params.devices  = 1

[deploy]
target = 'Linux'
converter.kind = 'QualiaCodeGen'
quantize = ['float32']
optimize = ['']
compress = [1]
#limit = 50

[dataset]
kind = "CIFAR10"
params.path = "/home/jupyter-tlouis/shared/cifar-10-batches-py"
params.dtype = "uint8" # Keep uint8 dtype instead of converting to float32 for AutoAugment

#[[preprocessing]]
#kind = "Normalize"
#params.method = 'z-score'
#params.axis = 0
#params.debug = true


[[data_augmentation]]
kind = "AutoAugment"
params.policy = "CIFAR10"
params.before = false
params.after = true
#params.before = true
#params.after = false

# Convert to Float32 and scale by 255 after AutoAugment
[[data_augmentation]]
kind = "IntToFloat32"
params.scale = true
params.before = false
params.after = true
params.evaluate = true

[[data_augmentation]]
kind = "HorizontalFlip"
params.before = false
params.after = true

[[data_augmentation]]
kind = "Crop"
params.size = [32, 32]
params.padding = [4, 4]
params.before = false
params.after = true

#[[data_augmentation]]
#kind = "GaussianNoise"
#params.sigma = 0.005


#[[data_augmentation]]
#kind = "Rotation2D"
#params.angle = [-5, 5]

#[[data_augmentation]]
#kind = "ResizedCrop"
#params.size = [32, 32]
#params.scale = [0.85, 1.05]
#params.ratio = [0.9, 1.1]


[[data_augmentation]]
kind = "Mixup"
params.before = false
params.after = true

[[preprocessing]]
kind = "Class2BinMatrix"

[[postprocessing]]

kind                = "QuantizationAwareTraining"
export              = true
params.epochs       = 30
params.batch_size   = 512
params.model.params.quantize_linear             = true
params.model.params.quant_params.bits           = 8
params.model.params.quant_params.quantype       = "fake"
params.model.params.quant_params.roundtype      = "nearest"
params.model.params.quant_params.range_setting  = "minmax"
params.model.params.quant_params.input.quant_enable=false
params.model.params.quant_params.LSQ            = false
params.model.params.quant_params.bias.quant_enable=true

[model_template]
kind                = "CNN"
epochs              = 30
load                = true
train               = false
batch_size          = 512
params.dims		    = 2
params.prepool 		= 1
#params.input_shape  = [32, 32, 3]
params.batch_norm	= false

[model_template.optimizer]
kind = "SGD"
#kind = "Adam"
#params.lr               = 0.0025
params.lr               = 0.075
#params.lr               = 0.05
params.momentum		= 0.9
#params.weight_decay	= 5e-4

[model_template.optimizer.scheduler]
kind            ='CosineAnnealingLR'
params.T_max    = 5

# With no Fuse - Q4

[[model]]
name = "cifar10_VGG-16-D_fused"
params.filters      = [ 64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512]
params.kernel_sizes = [  3,  3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3]
params.paddings     = [  1,  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]
params.strides      = [  1,  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]
params.pool_sizes   = [  0,  2,   0,   2,   0,   0,   2,   0,   0,   2,   0,   0,   2]
params.dropouts     = [  0,  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0.5, 0.5]
#params.fc_units     = [4096, 4096] # Original VGG
params.fc_units     = [512, 512] # Modified for CIFAR10
params.batch_norm 	= false
update.quant_params.bits    = 8
disabled = false

[model.optimizer]
kind = "SGD"
#kind = "Adam"
#params.lr               = 0.0025
params.lr               = 0.0075
#params.lr               = 0.05
params.momentum		= 0.9
params.weight_decay	= 0.01

[model.optimizer.scheduler]
kind            ='CosineAnnealingLR'
params.T_max    = 5