[bench]
name = "UCI-HAR_ResNetv1_QualiaCodeGen_Linux_int16_directpytorch"
first_run = 1
last_run = 1
seed = 2

[learningframework]
kind = 'PyTorch'
params.enable_progress_bar = false

[deploy]
target = 'Linux'
converter.kind = 'QualiaCodeGen'
quantize = ['int16']
optimize = ['']
compress = [1]

[dataset]
kind = "UCI_HAR"
params.variant = "raw"
params.path = "data/UCI HAR Dataset/"

[[preprocessing]]
kind = "DatamodelConverter"

[[preprocessing]]
kind = "Class2BinMatrix"

[[preprocessing]]
kind = "Normalize"
params.method = 'z-score'
params.axis = 0
params.debug = true

[[data_augmentation]]
kind = "Mixup"

# Not actually doing QAT since epochs=0, only used to generate activation ranges for QualiaCodeGen
[[postprocessing]]
kind = "QuantizationAwareTrainingFX"
export = true
params.epochs = 0
params.batch_size = 64
params.model.params.quant_params.bits           = 16
params.model.params.quant_params.quantype       = "fxp"
params.model.params.quant_params.roundtype      = "floor"
params.model.params.quant_params.range_setting  = "minmax"
params.model.params.quant_params.LSQ            = false
params.model.params.quant_params.force_q        = 9

[model_template]
kind = "ResNet"
epochs = 0
batch_size = 32
params.prepool 		= 1
params.strides		= [1, 2]
params.num_blocks	= [2]
load = true
train = false

[model_template.optimizer]
kind = "Adam"

[[model]]
name = "uci-har_resnetv1_8"
params.filters 		= [8, 8]
params.kernel_sizes 	= [3, 3]
params.paddings		= [1, 1]
params.batch_norm 	= false
disabled = false
